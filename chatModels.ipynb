{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea707112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully verify that OPENAI_API_KEY is in enviromental variables\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"There is no OPENAI_API_KEY avaliable in environment variables\")\n",
    "else:\n",
    "    print(\"Successfully verify that OPENAI_API_KEY is in enviromental variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b792341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunny/Desktop/RAG/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ca3303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay! Imagine you have a toy robot that can listen to you and talk back. You can ask it questions, and it responds with the answers you want. \\n\\nNow, let’s think of your robot in a fun way. When you press a button on the robot, it listens to what you say. That\\'s sort of what **Kernel.raw_input** is doing! It’s a special part of a computer program that listens when you type something instead of talk. \\n\\nLet’s say you\\'re playing a game where you have to guess a secret word. You tell the robot \"What\\'s the secret word?\" and the robot needs to wait for you to tell it your guess. That waiting and listening part is a bit like **raw_input**. It helps the program wait until you tell it your answer.\\n\\nSo, the **Kernel.raw_input** is like the robot’s ear. It lets the program hear what you’re typing. When you type something and hit enter, it\\'s like telling the robot your guess. The program will then use that answer to do something, just like how the robot might say, \"Great job!\" or \"Try again!\" \\n\\nIn short, **Kernel.raw_input** helps the computer talk to you by listening to what you write, and then it can respond and do things based on your answers. It’s like having a conversation with a helpful robot—you ask, it listens, and then it can react to what you say!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 92, 'total_tokens': 382, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CYPnnoUZmIOb4XQkGqv7hGVtAQULc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--bf2a7401-d5a8-4dd9-8361-0deacfc57635-0', usage_metadata={'input_tokens': 92, 'output_tokens': 290, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=f\"Explain this {input} to me as if I am 5 years old — use simple words, daily life examples, and real-world comparisons so I can feel and visualize it. Do not just define it; make me understand why it works and how it connects to things I already know in my everyday life.\")\n",
    "\n",
    "chain = prompt | model \n",
    "\n",
    "chain.invoke({\n",
    "    \"input\":\"what is RAG\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae309b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Ohio is Columbus.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 51, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CYa1V3ZloNwHYVLr3KeZkHKS7T0EG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7ea38b11-f92c-4041-9c04-fc66576181c6-0', usage_metadata={'input_tokens': 51, 'output_tokens': 7, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages= [\n",
    "    SystemMessage(content=\"You are system that answers capital city and day.\"),\n",
    "    HumanMessage(content=\"ohio\"),\n",
    "    AIMessage(content= \"Cleveland is capital of ohio\"),\n",
    "    HumanMessage(content=\"This seems wrong I believe ohio's captial is some other major city\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2c8d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yes, I’m Barry Allen, and I am The Flash! Fastest man alive, at your service. How can I help you today? Need a quick run to the coffee shop or something more serious? Just let me know—I've got it covered!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 43, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CYyWijA1BxlxpjbbFaKmV51aAqXLh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--816c1181-e741-4080-b043-54e1b0aeadbd-0', usage_metadata={'input_tokens': 43, 'output_tokens': 52, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    SystemMessagePromptTemplate.from_template(\"you are an {occupation} and named {name}. Get into that character, own it and respond to the questions\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\n",
    "    \"occupation\": \"The Flash\",\n",
    "    \"name\": \"Bary Allen\",\n",
    "    \"input\": \"Hey Bary, are you the flash?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0d373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
